services:
  verba:
    build:
      context: ./
      dockerfile: Dockerfile
    ports:
      - 8001:8000
    environment:
      - WEAVIATE_URL_VERBA=http://weaviate:8081
      - OPENAI_API_KEY=$OPENAI_API_KEY
      - COHERE_API_KEY=$COHERE_API_KEY
      - OLLAMA_URL=http://ollama:11434
      - OLLAMA_MODEL=llama2:70b
      - OLLAMA_EMBED_MODEL=$OLLAMA_EMBED_MODEL
      - UNSTRUCTURED_API_KEY=$UNSTRUCTURED_API_KEY
      - UNSTRUCTURED_API_URL=$UNSTRUCTURED_API_URL
      - GITHUB_TOKEN=$GITHUB_TOKEN
    volumes:
      - ./data:/data/
    depends_on:
      weaviate:
        condition: service_healthy
      ollama:
        condition: service_healthy
    healthcheck:
      test: wget --no-verbose --tries=3 --spider http://localhost:8001 || exit 1
      interval: 5s
      timeout: 10s
      retries: 5
      start_period: 10s
    networks:
      - ollama-docker
      - verba-network

  weaviate:
    command:
      - --host
      - 0.0.0.0
      - --port
      - '8081'
      - --scheme
      - http
    image: semitechnologies/weaviate:1.25.10
    ports:
      - 8081:8081
      - 3001:8081
    volumes:
      - weaviate_data:/var/lib/weaviate
    restart: on-failure:0
    healthcheck:
      test: wget --no-verbose --tries=3 --spider http://localhost:8081/v1/.well-known/ready || exit 1
      interval: 5s
      timeout: 10s
      retries: 5
      start_period: 10s
    environment:
      OPENAI_APIKEY: $OPENAI_API_KEY
      COHERE_APIKEY: $COHERE_API_KEY
      QUERY_DEFAULTS_LIMIT: 25
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'
      ENABLE_MODULES: 'e'
      CLUSTER_HOSTNAME: 'node1'
    networks:
      - verba-network

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_ORIGINS=*
      - OLLAMA_GPU_LAYERS=99
      - OLLAMA_CUDA_VISIBLE_DEVICES=all
      - OLLAMA_FLASH_ATTENTION=true
      - OLLAMA_GPU_MEMORY_UTILIZATION=0.95
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    volumes:
      - ollama_data:/root/.ollama
    healthcheck:
      test: ollama list || exit 1
      interval: 10s
      timeout: 30s
      retries: 5
      start_period: 30s
    command:
      - sh
      - -c
      - |
        ollama serve &
        SERVER_PID=$!
        
        until ollama list > /dev/null 2>&1; do
          echo "Waiting for Ollama service..."
          sleep 2
        done
        
        echo "Ollama service is ready!"
        
        if ! ollama list | grep -q 'mxbai-embed-large'; then
          echo 'Pulling mxbai-embed-large...'
          ollama pull mxbai-embed-large
        fi
        
        if ! ollama list | grep -q 'llama3.1:8b'; then
          echo 'Pulling llama3.1:8b...'
          ollama pull llama3.1:8b
        fi
        
        if ! ollama ps | grep -q 'llama3.1:8b'; then
          echo 'Initializing llama3.1:8b...'
          echo exit | ollama run llama3.1:8b
        fi
        
        wait $SERVER_PID

  ollama-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: ollama-webui
    volumes:
      - ./ollama/ollama-webui:/app/backend/data
    depends_on:
      - ollama
    ports:
      - 8083:8080  # Changed from 8082 to avoid conflict with traefik
    environment:
      - OLLAMA_BASE_URLS=http://ollama:11434
      - ENV=dev
      - WEBUI_AUTH=False
      - WEBUI_NAME=Verba AI
      - WEBUI_URL=http://localhost:8083  # Updated to match new port
      - WEBUI_SECRET_KEY=t0p-s3cr3t
    restart: unless-stopped
    networks:
      - ollama-docker

networks:
  ollama-docker:
    external: false
  verba-network:
    external: false

volumes:
  weaviate_data: { }
  ollama_data: { }