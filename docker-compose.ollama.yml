services:
  verba:
    build:
      context: ./
      dockerfile: Dockerfile
    ports:
      - 8001:8000
    environment:
      - WEAVIATE_URL_VERBA=http://weaviate:8081
      - OPENAI_API_KEY=$OPENAI_API_KEY
      - COHERE_API_KEY=$COHERE_API_KEY
      - OLLAMA_URL=http://ollama:11434
      - OLLAMA_MODEL=llama2:70b
      - OLLAMA_EMBED_MODEL=$OLLAMA_EMBED_MODEL
      - UNSTRUCTURED_API_KEY=$UNSTRUCTURED_API_KEY
      - UNSTRUCTURED_API_URL=$UNSTRUCTURED_API_URL
      - GITHUB_TOKEN=$GITHUB_TOKEN
    volumes:
      - ./data:/data/
    depends_on:
      weaviate:
        condition: service_healthy
      ollama:
        condition: service_healthy
    healthcheck:
      test: wget --no-verbose --tries=3 --spider http://localhost:8001 || exit 1
      interval: 5s
      timeout: 10s
      retries: 5
      start_period: 10s
    networks:
      - ollama-docker
      - verba-network

  weaviate:
    command:
      - --host
      - 0.0.0.0
      - --port
      - '8081'
      - --scheme
      - http
    image: semitechnologies/weaviate:1.25.10
    ports:
      - 8081:8081
      - 3001:8081
    volumes:
      - weaviate_data:/var/lib/weaviate
    restart: on-failure:0
    healthcheck:
      test: wget --no-verbose --tries=3 --spider http://localhost:8081/v1/.well-known/ready || exit 1
      interval: 5s
      timeout: 10s
      retries: 5
      start_period: 10s
    environment:
      OPENAI_APIKEY: $OPENAI_API_KEY
      COHERE_APIKEY: $COHERE_API_KEY
      QUERY_DEFAULTS_LIMIT: 25
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'
      ENABLE_MODULES: 'e'
      CLUSTER_HOSTNAME: 'node1'
    networks:
      - verba-network

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
    volumes:
      - ollama_data:/root/.ollama
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:11434/api/health" ]
      interval: 10s
      timeout: 5s
      retries: 5
    environment:
      - OLLAMA_ORIGINS=*
    command: >
      sh -c "
        ollama serve &
        sleep 10 &&
        if ! ollama list | grep -q 'mxbai-embed-large'; then
          echo 'Pulling mxbai-embed-large...'
          ollama pull mxbai-embed-large
        fi &&
        if ! ollama list | grep -q 'llama3.1:8b'; then
          echo 'Pulling llama3.1:8b...'
          ollama pull llama3.1:8b
        fi &&
        if ! ollama ps | grep -q 'llama3.1:8b'; then
          echo 'Initializing llama3.1:8b...'
          ollama run llama3.1:8b <<< 'exit'
        fi &&
        wait
      "

  ollama-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: ollama-webui
    volumes:
      - ./ollama/ollama-webui:/app/backend/data
    depends_on:
      - ollama
    ports:
      - 8083:8080  # Changed from 8082 to avoid conflict with traefik
    environment:
      - OLLAMA_BASE_URLS=http://ollama:11434
      - ENV=dev
      - WEBUI_AUTH=False
      - WEBUI_NAME=Verba AI
      - WEBUI_URL=http://localhost:8083  # Updated to match new port
      - WEBUI_SECRET_KEY=t0p-s3cr3t
    restart: unless-stopped
    networks:
      - ollama-docker

networks:
  ollama-docker:
    external: false
  verba-network:
    external: false

volumes:
  weaviate_data: { }
  ollama_data: { }